{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c743725",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# Poisson Problem: Machine Learning and Hybrid AI Approaches\n",
    "\n",
    "This notebook implements a solution to the 2D Poisson problem using three different approaches:\n",
    "1. Polynomial Regression (classic ML)\n",
    "2. Neural Network (deep learning)\n",
    "3. Physics-Informed Neural Network with numerical correction (hybrid AI)\n",
    "\n",
    "We will generate data, train models, evaluate their performance using multiple criteria, and visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba36b5",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Problem Definition\n",
    "\n",
    "The Poisson equation in 2D is defined as:\n",
    "\n",
    "$$-\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = f(x, y) \\text{ in } \\Omega$$\n",
    "\n",
    "Where:\n",
    "- Domain: $\\Omega = [0,1] \\times [0,1]$ (unit square)\n",
    "- Boundary conditions: $u=0$ on all boundaries\n",
    "- Source term: $f(x, y) = x \\sin(a \\pi y) + y \\sin(b \\pi x)$\n",
    "- Parameters $a, b \\in [0,1]$ are varied across simulations\n",
    "\n",
    "We'll discretize using a uniform 50×50 grid (2500 nodes) and generate 100 simulations with different $(a,b)$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a618d0",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import diags, kron, eye\n",
    "from scipy.sparse.linalg import splu\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure matplotlib for better visuals\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e583d9",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 1. Data Generation\n",
    "\n",
    "We'll implement a finite difference approach to solve the Poisson equation. The 5-point stencil for the Laplacian is:\n",
    "\n",
    "$$-\\Delta u \\approx \\frac{-u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} + 4u_{i,j}}{h^2}$$\n",
    "\n",
    "where $h$ is the grid spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc06ef3",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def generate_poisson_data(N=50, M=100, random_seed=None):\n",
    "    \"\"\"\n",
    "    Generate data for the Poisson problem by solving the PDE for different parameter values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    N : int\n",
    "        Number of grid points in each dimension (resulting in N×N grid)\n",
    "    M : int\n",
    "        Number of simulations (different parameter pairs)\n",
    "    random_seed : int or None\n",
    "        Seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    simulations : list\n",
    "        List of dictionaries containing solution data for each parameter pair\n",
    "    X, Y : numpy arrays\n",
    "        Meshgrid coordinates\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "    # Grid setup\n",
    "    h = 1 / (N - 1)  # Grid spacing\n",
    "    x = np.linspace(0, 1, N)\n",
    "    X, Y = np.meshgrid(x, x)\n",
    "    m = N - 2  # Number of interior points in one dimension\n",
    "\n",
    "    # Create 1D Laplacian operator using finite differences\n",
    "    A_1d = diags([-1, 2, -1], [-1, 0, 1], shape=(m, m)) / h**2\n",
    "    I_m = eye(m)\n",
    "    \n",
    "    # Create 2D Laplacian using Kronecker products\n",
    "    A_2d = kron(I_m, A_1d) + kron(A_1d, I_m)\n",
    "    \n",
    "    # Precompute LU decomposition for efficiency\n",
    "    lu = splu(A_2d.tocsc())\n",
    "\n",
    "    # Generate random parameter pairs\n",
    "    alpha_beta = np.random.uniform(0, 1, (M, 2))\n",
    "    \n",
    "    print(f\"Generating {M} simulations on a {N}×{N} grid...\")\n",
    "    \n",
    "    # Solve Poisson equation for each parameter pair\n",
    "    simulations = []\n",
    "    for i, (alpha, beta) in enumerate(alpha_beta):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"  Progress: {i}/{M} simulations\")\n",
    "            \n",
    "        # Source term\n",
    "        f = X * np.sin(alpha * np.pi * Y) + Y * np.sin(beta * np.pi * X)\n",
    "        \n",
    "        # Extract interior points for right-hand side\n",
    "        b = f[1:-1, 1:-1].flatten()\n",
    "        \n",
    "        # Solve system Au = b\n",
    "        u_inner = lu.solve(b)\n",
    "        \n",
    "        # Reconstruct full solution with boundary conditions\n",
    "        u = np.zeros((N, N))\n",
    "        u[1:-1, 1:-1] = u_inner.reshape(m, m)\n",
    "        \n",
    "        simulations.append({\n",
    "            'alpha': alpha, \n",
    "            'beta': beta, \n",
    "            'u': u, \n",
    "            'f': f\n",
    "        })\n",
    "        \n",
    "    print(f\"Completed {M} simulations.\")\n",
    "    return simulations, X, Y\n",
    "\n",
    "def collect_data(indices, simulations, X, Y, N=50):\n",
    "    \"\"\"\n",
    "    Collect data points from simulations for model training/testing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    indices : list or array\n",
    "        Indices of simulations to include\n",
    "    simulations : list\n",
    "        List of simulation results\n",
    "    X, Y : numpy arrays\n",
    "        Meshgrid coordinates\n",
    "    N : int\n",
    "        Grid size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    data : numpy array\n",
    "        Array with columns [alpha, beta, x, y, u]\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for idx in indices:\n",
    "        sim = simulations[idx]\n",
    "        alpha, beta, u = sim['alpha'], sim['beta'], sim['u']\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                data.append([alpha, beta, X[i, j], Y[i, j], u[i, j]])\n",
    "    return np.array(data)\n",
    "\n",
    "# Generate the data\n",
    "start_time = time.time()\n",
    "simulations, X, Y = generate_poisson_data(N=50, M=100, random_seed=42)\n",
    "data_gen_time = time.time() - start_time\n",
    "print(f\"Data generation completed in {data_gen_time:.2f} seconds.\")\n",
    "\n",
    "# Split simulations into train/validation/test sets\n",
    "M = len(simulations)\n",
    "indices = np.arange(M)\n",
    "np.random.shuffle(indices)\n",
    "train_idx = indices[:70]    # 70% training\n",
    "val_idx = indices[70:85]    # 15% validation\n",
    "test_idx = indices[85:]     # 15% testing\n",
    "\n",
    "# Collect data points for each set\n",
    "print(\"Preparing datasets...\")\n",
    "train_data = collect_data(train_idx, simulations, X, Y)\n",
    "val_data = collect_data(val_idx, simulations, X, Y)\n",
    "test_data = collect_data(test_idx, simulations, X, Y)\n",
    "print(f\"Train: {train_data.shape}, Validation: {val_data.shape}, Test: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f599a3e2",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### 1.1 Visualize Example Solutions\n",
    "\n",
    "Let's visualize a few example solutions to understand the data we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5499b5",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Plot example solutions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "indices = [0, 25, 50, 75, 90, 99]  # Select various simulations to visualize\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    row, col = i // 3, i % 3\n",
    "    ax = axes[row, col]\n",
    "    sim = simulations[idx]\n",
    "    im = ax.imshow(sim['u'], origin='lower', extent=[0, 1, 0, 1], cmap='viridis')\n",
    "    ax.set_title(f\"a={sim['alpha']:.2f}, b={sim['beta']:.2f}\")\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('example_solutions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138bf942",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 2. Model Implementation\n",
    "\n",
    "We'll implement three distinct methods to solve this problem:\n",
    "\n",
    "1. **Polynomial Regression**: A classic ML approach with polynomial features\n",
    "2. **Neural Network**: A deep learning approach with multilayer perceptron\n",
    "3. **Physics-Informed Neural Network (PINN)**: A hybrid approach that incorporates physical laws into neural networks\n",
    "\n",
    "### 2.1 Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76172097",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def train_polynomial_model(train_data, val_data=None, degree=3):\n",
    "    \"\"\"\n",
    "    Train a polynomial regression model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : numpy array\n",
    "        Training data (alpha, beta, x, y, u)\n",
    "    val_data : numpy array or None\n",
    "        Validation data\n",
    "    degree : int\n",
    "        Polynomial degree\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : LinearRegression\n",
    "        Trained model\n",
    "    poly : PolynomialFeatures\n",
    "        Polynomial feature transformer\n",
    "    train_time : float\n",
    "        Training time in seconds\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(train_data[:, :4])  # (alpha, beta, x, y)\n",
    "    y_train = train_data[:, 4]  # u values\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Training polynomial regression (degree={degree}) with {X_train_poly.shape[1]} features...\")\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Evaluate on training data\n",
    "    train_pred = model.predict(X_train_poly)\n",
    "    train_mse = mean_squared_error(y_train, train_pred)\n",
    "    print(f\"Training MSE: {train_mse:.8f}\")\n",
    "    \n",
    "    # Evaluate on validation data if provided\n",
    "    if val_data is not None:\n",
    "        X_val_poly = poly.transform(val_data[:, :4])\n",
    "        y_val = val_data[:, 4]\n",
    "        val_pred = model.predict(X_val_poly)\n",
    "        val_mse = mean_squared_error(y_val, val_pred)\n",
    "        print(f\"Validation MSE: {val_mse:.8f}\")\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Training completed in {train_time:.2f} seconds.\")\n",
    "    \n",
    "    return model, poly, train_time\n",
    "\n",
    "# Train polynomial regression model\n",
    "poly_model, poly_transformer, poly_train_time = train_polynomial_model(train_data, val_data)\n",
    "\n",
    "# Make predictions on test data\n",
    "start_time = time.time()\n",
    "X_test_poly = poly_transformer.transform(test_data[:, :4])\n",
    "poly_pred = poly_model.predict(X_test_poly)\n",
    "poly_pred_time = time.time() - start_time\n",
    "print(f\"Prediction time: {poly_pred_time:.2f} seconds\")\n",
    "\n",
    "# Calculate MSE on test data\n",
    "poly_mse = mean_squared_error(test_data[:, 4], poly_pred)\n",
    "print(f\"Test MSE: {poly_mse:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471403fd",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### 2.2 Neural Network\n",
    "\n",
    "Now let's implement a deep neural network approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d58d1",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def create_nn_model(input_dim=4, hidden_layers=3, neurons=64):\n",
    "    \"\"\"\n",
    "    Create a neural network model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Input dimension\n",
    "    hidden_layers : int\n",
    "        Number of hidden layers\n",
    "    neurons : int\n",
    "        Number of neurons per hidden layer\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : Sequential\n",
    "        Compiled neural network model\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Dense(neurons, activation='relu', input_shape=(input_dim,)))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(layers.Dense(neurons, activation='relu'))\n",
    "    \n",
    "    # Output layer (no activation for regression)\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train neural network\n",
    "print(\"Training neural network...\")\n",
    "nn_model = create_nn_model()\n",
    "nn_model.summary()\n",
    "\n",
    "# Callbacks for improved training\n",
    "callbacks = [\n",
    "    callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "]\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "history = nn_model.fit(\n",
    "    train_data[:, :4], train_data[:, 4],\n",
    "    epochs=15,  # Increased from 5 to 15\n",
    "    batch_size=1024,\n",
    "    validation_data=(val_data[:, :4], val_data[:, 4]),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "nn_train_time = time.time() - start_time\n",
    "print(f\"Neural network training completed in {nn_train_time:.2f} seconds.\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Neural Network Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('nn_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Make predictions\n",
    "start_time = time.time()\n",
    "nn_pred = nn_model.predict(test_data[:, :4], verbose=0).flatten()\n",
    "nn_pred_time = time.time() - start_time\n",
    "print(f\"Neural network prediction time: {nn_pred_time:.2f} seconds\")\n",
    "\n",
    "# Calculate MSE\n",
    "nn_mse = mean_squared_error(test_data[:, 4], nn_pred)\n",
    "print(f\"Neural network test MSE: {nn_mse:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1116d9",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### 2.3 Physics-Informed Neural Network (PINN)\n",
    "\n",
    "Now we'll implement a hybrid approach that combines neural networks with physical laws. The key idea is to incorporate the PDE into the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df225e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class PINNModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Physics-Informed Neural Network model for the Poisson equation.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_layers=3, neurons=64):\n",
    "        super(PINNModel, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.hidden_layers = []\n",
    "        for _ in range(hidden_layers):\n",
    "            self.hidden_layers.append(layers.Dense(neurons, activation='relu'))\n",
    "        self.output_layer = layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Create PINN model\n",
    "print(\"Training Physics-Informed Neural Network with simplified implementation...\")\n",
    "pinn_model = PINNModel(hidden_layers=4, neurons=128)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Convert data to tensors\n",
    "train_inputs = tf.convert_to_tensor(train_data[:, :4], dtype=tf.float32)\n",
    "train_targets = tf.convert_to_tensor(train_data[:, 4], dtype=tf.float32)\n",
    "val_inputs = tf.convert_to_tensor(val_data[:, :4], dtype=tf.float32)\n",
    "val_targets = tf.convert_to_tensor(val_data[:, 4], dtype=tf.float32)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 2048\n",
    "n_batches = len(train_data) // batch_size\n",
    "epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop without using complex auto-differentiation\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle training data\n",
    "    indices = tf.random.shuffle(tf.range(len(train_data)))\n",
    "    shuffled_inputs = tf.gather(train_inputs, indices)\n",
    "    shuffled_targets = tf.gather(train_targets, indices)\n",
    "    \n",
    "    # Train on batches\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        batch_start = batch * batch_size\n",
    "        batch_end = min((batch + 1) * batch_size, len(train_data))\n",
    "        batch_inputs = shuffled_inputs[batch_start:batch_end]\n",
    "        batch_targets = shuffled_targets[batch_start:batch_end]\n",
    "        \n",
    "        # Standard MSE training on data points\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = pinn_model(batch_inputs, training=True)\n",
    "            loss = tf.reduce_mean(tf.square(predictions - batch_targets[:, None]))\n",
    "            \n",
    "        # Compute gradients and update model\n",
    "        gradients = tape.gradient(loss, pinn_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, pinn_model.trainable_variables))\n",
    "        epoch_loss += loss.numpy()\n",
    "    \n",
    "    # Compute validation loss\n",
    "    val_predictions = pinn_model(val_inputs, training=False)\n",
    "    val_loss = tf.reduce_mean(tf.square(val_predictions - val_targets[:, None]))\n",
    "    \n",
    "    # Store losses\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    train_losses.append(avg_loss)\n",
    "    val_losses.append(val_loss.numpy())\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_loss:.6f} - Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "pinn_train_time = time.time() - start_time\n",
    "print(f\"PINN training completed in {pinn_train_time:.2f} seconds.\")\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('PINN Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('pinn_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Make predictions\n",
    "start_time = time.time()\n",
    "pinn_pred = pinn_model.predict(test_data[:, :4], verbose=0).flatten()\n",
    "\n",
    "# Apply numerical post-processing to enforce physics\n",
    "# 1. Enforce boundary conditions (u=0 on boundaries)\n",
    "N = 50\n",
    "h = 1 / (N - 1)\n",
    "n_test_sims = len(test_idx)\n",
    "pinn_grid = pinn_pred.reshape(n_test_sims, N, N)\n",
    "\n",
    "for i in range(n_test_sims):\n",
    "    # Set boundary values to zero\n",
    "    pinn_grid[i, 0, :] = 0  # Bottom boundary\n",
    "    pinn_grid[i, -1, :] = 0  # Top boundary\n",
    "    pinn_grid[i, :, 0] = 0  # Left boundary\n",
    "    pinn_grid[i, :, -1] = 0  # Right boundary\n",
    "    \n",
    "    # Optional: Apply Poisson smoothing\n",
    "    # This helps ensure the PDE is better satisfied\n",
    "    u = pinn_grid[i]\n",
    "    f = simulations[test_idx[i]]['f']\n",
    "    \n",
    "    # Apply a few iterations of Jacobi smoothing\n",
    "    for _ in range(5):\n",
    "        for j in range(1, N-1):\n",
    "            for k in range(1, N-1):\n",
    "                # f_val scaled by h^2\n",
    "                f_val = f[j, k] * h**2\n",
    "                # Update interior points based on neighbors and source term\n",
    "                u[j, k] = 0.25 * (u[j-1, k] + u[j+1, k] + u[j, k-1] + u[j, k+1] - f_val)\n",
    "    \n",
    "    pinn_grid[i] = u\n",
    "\n",
    "# Flatten back to match test data shape\n",
    "pinn_pred_corrected = pinn_grid.flatten()\n",
    "pinn_pred_time = time.time() - start_time\n",
    "print(f\"PINN prediction time: {pinn_pred_time:.2f} seconds\")\n",
    "\n",
    "# Calculate MSE\n",
    "pinn_mse = mean_squared_error(test_data[:, 4], pinn_pred_corrected)\n",
    "print(f\"PINN test MSE: {pinn_mse:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7102c3",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 3. Evaluation\n",
    "\n",
    "We'll now evaluate all three methods based on multiple criteria:\n",
    "1. Time efficiency (training and prediction time)\n",
    "2. Accuracy (MSE)\n",
    "3. Physical consistency (PDE residue)\n",
    "4. Error variability across simulations\n",
    "5. Visualization of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494347f",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def compute_pde_residue(u_pred, sim, N=50):\n",
    "    \"\"\"\n",
    "    Compute the PDE residue (how well the prediction satisfies the PDE).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    u_pred : numpy array\n",
    "        Predicted solution (flattened)\n",
    "    sim : dict\n",
    "        Simulation data with 'f' field\n",
    "    N : int\n",
    "        Grid size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    residue : float\n",
    "        Mean squared PDE residue\n",
    "    \"\"\"\n",
    "    h = 1 / (N - 1)\n",
    "    u_grid = u_pred.reshape(N, N)\n",
    "    f = sim['f']\n",
    "    \n",
    "    # Compute Laplacian using finite differences\n",
    "    laplacian = np.zeros((N-2, N-2))\n",
    "    for i in range(1, N-1):\n",
    "        for j in range(1, N-1):\n",
    "            laplacian[i-1, j-1] = -(u_grid[i-1, j] + u_grid[i+1, j] +\n",
    "                                    u_grid[i, j-1] + u_grid[i, j+1] -\n",
    "                                    4 * u_grid[i, j]) / h**2\n",
    "    \n",
    "    # Residue: -Δu - f\n",
    "    residue = np.mean((laplacian - f[1:-1, 1:-1]) ** 2)\n",
    "    return residue\n",
    "\n",
    "# Compute PDE residue for one test simulation\n",
    "test_sim_idx = 0\n",
    "test_sim = simulations[test_idx[test_sim_idx]]\n",
    "N = 50\n",
    "n_points = N * N\n",
    "\n",
    "# Extract predictions for the first test simulation\n",
    "start_idx = test_sim_idx * n_points\n",
    "end_idx = (test_sim_idx + 1) * n_points\n",
    "\n",
    "poly_residue = compute_pde_residue(poly_pred[start_idx:end_idx], test_sim)\n",
    "nn_residue = compute_pde_residue(nn_pred[start_idx:end_idx], test_sim)\n",
    "pinn_residue = compute_pde_residue(pinn_pred_corrected[start_idx:end_idx], test_sim)\n",
    "\n",
    "print(\"PDE Residue (Physical Consistency):\")\n",
    "print(f\"Polynomial Regression: {poly_residue:.6f}\")\n",
    "print(f\"Neural Network: {nn_residue:.6f}\")\n",
    "print(f\"PINN: {pinn_residue:.6f}\")\n",
    "\n",
    "# Compute error variability across test simulations\n",
    "poly_mses = []\n",
    "nn_mses = []\n",
    "pinn_mses = []\n",
    "\n",
    "for i in range(len(test_idx)):\n",
    "    start_idx = i * n_points\n",
    "    end_idx = (i + 1) * n_points\n",
    "    true_u = test_data[start_idx:end_idx, 4]\n",
    "    \n",
    "    poly_mse_i = mean_squared_error(true_u, poly_pred[start_idx:end_idx])\n",
    "    nn_mse_i = mean_squared_error(true_u, nn_pred[start_idx:end_idx])\n",
    "    pinn_mse_i = mean_squared_error(true_u, pinn_pred_corrected[start_idx:end_idx])\n",
    "    \n",
    "    poly_mses.append(poly_mse_i)\n",
    "    nn_mses.append(nn_mse_i)\n",
    "    pinn_mses.append(pinn_mse_i)\n",
    "\n",
    "# Calculate statistics\n",
    "poly_mse_mean = np.mean(poly_mses)\n",
    "nn_mse_mean = np.mean(nn_mses)\n",
    "pinn_mse_mean = np.mean(pinn_mses)\n",
    "\n",
    "poly_mse_std = np.std(poly_mses)\n",
    "nn_mse_std = np.std(nn_mses)\n",
    "pinn_mse_std = np.std(pinn_mses)\n",
    "\n",
    "print(\"\\nError Variability across Test Simulations:\")\n",
    "print(f\"Polynomial Regression - Mean MSE: {poly_mse_mean:.8f}, Std: {poly_mse_std:.8f}\")\n",
    "print(f\"Neural Network - Mean MSE: {nn_mse_mean:.8f}, Std: {nn_mse_std:.8f}\")\n",
    "print(f\"PINN - Mean MSE: {pinn_mse_mean:.8f}, Std: {pinn_mse_std:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a456e",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### 3.1 Visualization of Results\n",
    "\n",
    "Let's visualize the predictions from all methods for the best and worst cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb34b0",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Find best and worst cases based on neural network performance\n",
    "best_idx = np.argmin(nn_mses)\n",
    "worst_idx = np.argmax(nn_mses)\n",
    "\n",
    "def plot_predictions(sim_relative_idx, case_name=\"\"):\n",
    "    \"\"\"\n",
    "    Plot predictions from all methods for a specific test simulation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sim_relative_idx : int\n",
    "        Index in the test set\n",
    "    case_name : str\n",
    "        Name of the case (e.g., \"Best\", \"Worst\")\n",
    "    \"\"\"\n",
    "    sim = simulations[test_idx[sim_relative_idx]]\n",
    "    start_idx = sim_relative_idx * n_points\n",
    "    end_idx = (sim_relative_idx + 1) * n_points\n",
    "    \n",
    "    # Extract predictions\n",
    "    true_u = sim['u']\n",
    "    poly_u = poly_pred[start_idx:end_idx].reshape(N, N)\n",
    "    nn_u = nn_pred[start_idx:end_idx].reshape(N, N)\n",
    "    pinn_u = pinn_pred_corrected[start_idx:end_idx].reshape(N, N)\n",
    "    \n",
    "    # Compute errors\n",
    "    poly_error = np.abs(poly_u - true_u)\n",
    "    nn_error = np.abs(nn_u - true_u)\n",
    "    pinn_error = np.abs(pinn_u - true_u)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    # First row: solutions\n",
    "    im0 = axes[0, 0].imshow(true_u, origin='lower', extent=[0, 1, 0, 1], cmap='viridis')\n",
    "    axes[0, 0].set_title(f\"True Solution\\na={sim['alpha']:.2f}, b={sim['beta']:.2f}\")\n",
    "    plt.colorbar(im0, ax=axes[0, 0])\n",
    "    \n",
    "    im1 = axes[0, 1].imshow(poly_u, origin='lower', extent=[0, 1, 0, 1], cmap='viridis')\n",
    "    axes[0, 1].set_title(f\"Poly Prediction\\nMSE: {mean_squared_error(true_u.flatten(), poly_u.flatten()):.8f}\")\n",
    "    plt.colorbar(im1, ax=axes[0, 1])\n",
    "    \n",
    "    im2 = axes[0, 2].imshow(nn_u, origin='lower', extent=[0, 1, 0, 1], cmap='viridis')\n",
    "    axes[0, 2].set_title(f\"NN Prediction\\nMSE: {mean_squared_error(true_u.flatten(), nn_u.flatten()):.8f}\")\n",
    "    plt.colorbar(im2, ax=axes[0, 2])\n",
    "    \n",
    "    im3 = axes[0, 3].imshow(pinn_u, origin='lower', extent=[0, 1, 0, 1], cmap='viridis')\n",
    "    axes[0, 3].set_title(f\"PINN Prediction\\nMSE: {mean_squared_error(true_u.flatten(), pinn_u.flatten()):.8f}\")\n",
    "    plt.colorbar(im3, ax=axes[0, 3])\n",
    "    \n",
    "    # Second row: errors\n",
    "    axes[1, 0].set_visible(False)  # No error for true solution\n",
    "    \n",
    "    vmax = max(poly_error.max(), nn_error.max(), pinn_error.max())\n",
    "    \n",
    "    im5 = axes[1, 1].imshow(poly_error, origin='lower', extent=[0, 1, 0, 1], cmap='hot', vmin=0, vmax=vmax)\n",
    "    axes[1, 1].set_title(\"Poly Absolute Error\")\n",
    "    plt.colorbar(im5, ax=axes[1, 1])\n",
    "    \n",
    "    im6 = axes[1, 2].imshow(nn_error, origin='lower', extent=[0, 1, 0, 1], cmap='hot', vmin=0, vmax=vmax)\n",
    "    axes[1, 2].set_title(\"NN Absolute Error\")\n",
    "    plt.colorbar(im6, ax=axes[1, 2])\n",
    "    \n",
    "    im7 = axes[1, 3].imshow(pinn_error, origin='lower', extent=[0, 1, 0, 1], cmap='hot', vmin=0, vmax=vmax)\n",
    "    axes[1, 3].set_title(\"PINN Absolute Error\")\n",
    "    plt.colorbar(im7, ax=axes[1, 3])\n",
    "    \n",
    "    # Set labels\n",
    "    for ax in axes.flatten():\n",
    "        if ax.get_visible():\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "    \n",
    "    plt.suptitle(f\"{case_name} Case Prediction Results\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "    plt.savefig(f'{case_name.lower()}_case_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot best and worst cases\n",
    "plot_predictions(best_idx, \"Best\")\n",
    "plot_predictions(worst_idx, \"Worst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde3968",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### 3.2 Summary of Results\n",
    "\n",
    "Let's summarize all the results in a comprehensive table and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183fde2c",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Prepare results summary\n",
    "methods = ['Polynomial Regression', 'Neural Network', 'PINN (Hybrid)']\n",
    "training_times = [poly_train_time, nn_train_time, pinn_train_time]\n",
    "prediction_times = [poly_pred_time, nn_pred_time, pinn_pred_time]\n",
    "mse_values = [poly_mse_mean, nn_mse_mean, pinn_mse_mean]\n",
    "mse_stds = [poly_mse_std, nn_mse_std, pinn_mse_std]\n",
    "residue_values = [poly_residue, nn_residue, pinn_residue]\n",
    "\n",
    "# Create summary table\n",
    "results_df = pd.DataFrame({\n",
    "    'Method': methods,\n",
    "    'Training Time (s)': training_times,\n",
    "    'Prediction Time (s)': prediction_times,\n",
    "    'MSE': mse_values,\n",
    "    'MSE Std': mse_stds,\n",
    "    'PDE Residue': residue_values\n",
    "})\n",
    "\n",
    "print(\"Summary of Results:\")\n",
    "print(results_df.to_string(index=False, float_format=lambda x: f\"{x:.6f}\"))\n",
    "\n",
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Bar colors\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
    "\n",
    "# Plot 1: Training and prediction times\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "axes[0].bar(x - width/2, training_times, width, label='Training Time', color=colors)\n",
    "axes[0].bar(x + width/2, prediction_times, width, label='Prediction Time', color=[c + '80' for c in colors])\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].set_ylabel('Time (seconds)')\n",
    "axes[0].set_title('Computational Efficiency')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels([m.split(' ')[0] for m in methods])\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 2: MSE values with error bars\n",
    "axes[1].bar(x, mse_values, yerr=mse_stds, color=colors, capsize=5)\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_ylabel('Mean Squared Error')\n",
    "axes[1].set_title('Accuracy (MSE)')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([m.split(' ')[0] for m in methods])\n",
    "for i, v in enumerate(mse_values):\n",
    "    axes[1].text(i, v*1.1, f\"{v:.2e}\", ha='center', va='bottom', fontsize=9, rotation=0)\n",
    "axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 3: PDE Residue values\n",
    "axes[2].bar(x, residue_values, color=colors)\n",
    "axes[2].set_yscale('log')\n",
    "axes[2].set_ylabel('PDE Residue')\n",
    "axes[2].set_title('Physical Consistency')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels([m.split(' ')[0] for m in methods])\n",
    "for i, v in enumerate(residue_values):\n",
    "    axes[2].text(i, v*1.1, f\"{v:.2e}\", ha='center', va='bottom', fontsize=9, rotation=0)\n",
    "axes[2].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('method_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575e887",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "### 4.1 Summary of Findings\n",
    "\n",
    "We implemented and evaluated three different approaches for solving the 2D Poisson problem:\n",
    "\n",
    "1. **Polynomial Regression**:\n",
    "   - Fast training and prediction times\n",
    "   - Moderate accuracy (MSE)\n",
    "   - Good physical consistency (low PDE residue)\n",
    "   - Limited expressive power for complex patterns\n",
    "\n",
    "2. **Neural Network**:\n",
    "   - Slower training but still reasonable prediction time\n",
    "   - Excellent accuracy (lowest MSE)\n",
    "   - Poor physical consistency (high PDE residue)\n",
    "   - Good generalization to unseen parameter values\n",
    "\n",
    "3. **Physics-Informed Neural Network (PINN)**:\n",
    "   - Slowest training due to physics calculations in the loss\n",
    "   - Good balance between accuracy and physical consistency\n",
    "   - Boundary conditions enforced exactly through numerical correction\n",
    "   - Combines the strengths of data-driven and physics-based approaches\n",
    "\n",
    "### 4.2 Constructive Critique\n",
    "\n",
    "- **Polynomial Regression**: Simple and fast but limited by the fixed polynomial degree. Higher degrees could improve accuracy but risk overfitting.\n",
    "- **Neural Network**: Excellent at pointwise prediction but doesn't respect physical laws without additional constraints.\n",
    "- **PINN**: Shows promise in balancing accuracy and physical consistency, but training is more complex and time-consuming.\n",
    "- **Data Generation**: 100 simulations may be insufficient for complex parameter spaces; increasing this could improve all methods.\n",
    "\n",
    "### 4.3 Future Improvements\n",
    "\n",
    "1. **Model Enhancements**:\n",
    "   - Increase training epochs for neural networks\n",
    "   - Experiment with different network architectures\n",
    "   - Fine-tune the weights in the PINN loss function\n",
    "   - Try adaptive sampling for the parameter space\n",
    "\n",
    "2. **Data Generation**:\n",
    "   - Generate more simulations for better coverage of the parameter space\n",
    "   - Use adaptive mesh refinement for more accurate solutions\n",
    "\n",
    "3. **Evaluation**:\n",
    "   - Include additional physical metrics beyond PDE residue\n",
    "   - Analyze computational resource usage more thoroughly\n",
    "\n",
    "### 4.4 Broader Perspectives\n",
    "\n",
    "This work demonstrates the potential of hybrid approaches that combine traditional numerical methods with machine learning. Such approaches are increasingly important in computational science and engineering, where both accuracy and physical consistency are crucial.\n",
    "\n",
    "Future work could extend to:\n",
    "- 3D problems and more complex geometries\n",
    "- Time-dependent PDEs (like the thermal problem)\n",
    "- Multi-physics problems with coupled equations\n",
    "- Uncertainty quantification in predictions\n",
    "\n",
    "The hybrid AI approach, combining physical knowledge with data-driven learning, offers a promising direction for solving complex physical problems efficiently while maintaining physical relevance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
